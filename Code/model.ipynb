{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import Conv2D, Conv2DTranspose, LeakyReLU, Activation, Concatenate, BatchNormalization\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from random import randint, shuffle\n",
    "\n",
    "class CycleGAN():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # input image shape\n",
    "        self.image_rows = 128\n",
    "        self.image_cols = 128\n",
    "        self.channels = 3\n",
    "        self.image_shape = (self.image_rows, self.image_cols, self.channels)\n",
    "        \n",
    "        # load data\n",
    "        self.loadsize = 400\n",
    "        self.imagesize = self.image_rows \n",
    "        self.dpath = 'data/ShoeV2/'\n",
    "        \n",
    "        # hyper parameter\n",
    "        self.lr_D = 0.0002\n",
    "        self.lr_G = 0.0002\n",
    "        self.beta_1 = 0.5\n",
    "        self.batch_size = 1\n",
    "        self.epochs = 10\n",
    "        self.save_interval = 10\n",
    "        \n",
    "         # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.image_rows / 2**3)\n",
    "        self.patch_shape = (patch, patch, 1)\n",
    "        \n",
    "        self.use_patchgan = True\n",
    "        self.n_resnet = 6\n",
    "        \n",
    "        # Hyper parameters\n",
    "        # Cycle-consistency loss weights\n",
    "        self.lambda_forward_cycle = 10.0\n",
    "        self.lambda_backward_cycle = 10.0\n",
    "        \n",
    "        # build the two generators and two discriminators used in the CycleGAN\n",
    "        # Domain X -> Y\n",
    "        self.G = self.generator(name='G')\n",
    "        # Domain Y -> X\n",
    "        self.F = self.generator(name='F')\n",
    "        \n",
    "        # predict Y\n",
    "        self.D_y = self.discriminator(name='D_y')\n",
    "        # predict X\n",
    "        self.D_x = self.discriminator(name='D_x')\n",
    "        \n",
    "        # compile the discriminators to train discriminators \n",
    "        # In practice, we divide the objective by 2 while optimizing D, which slows down the rate at which D learns, relative to the rate of G\n",
    "        self.D_y.compile(loss='mse', optimizer=Adam(lr=self.lr_D, beta_1=self.beta_1), loss_weights=[0.5])\n",
    "        self.D_x.compile(loss='mse', optimizer=Adam(lr=self.lr_D, beta_1=self.beta_1), loss_weights=[0.5])\n",
    "\n",
    "        # compile the composite model to train generators to fool discriminators\n",
    "        self.Composite = self.composite_model(name='Composite')\n",
    "        self.Composite.compile(loss=['mse','mse','mae','mae'], optimizer=Adam(lr=self.lr_G, beta_1=self.beta_1), loss_weights=[1, 1, self.lambda_forward_cycle, self.lambda_backward_cycle])\n",
    "        \n",
    "        \n",
    "\n",
    "#===============================================================================\n",
    "# Architecture functions\n",
    "\n",
    "    # Ck denote a 4 × 4 Convolution-InstanceNorm-LeakyReLU layer with k ﬁlters and stride 2\n",
    "    def ck(self, layer_input, k, use_normalization, stride):\n",
    "        x = Conv2D(filters=k, kernel_size=4, strides=stride, padding='same')(layer_input)\n",
    "        if use_normalization:\n",
    "        # The “axis” argument is set to -1 to ensure that features are normalized per feature map\n",
    "            x = InstanceNormalization(axis=-1)(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        return x\n",
    "    \n",
    "    # Rk denotes a residual block that contains two 3 × 3 convolutional layers with the same number of ﬁlters on both layer\n",
    "    def Rk(self, layer_input, k):\n",
    "        # 1st layer\n",
    "        # Same padding is used instead of reflection padded recommended in the paper for simplicity\n",
    "        x = Conv2D(filters=k, kernel_size=3, strides=1, padding='same')(layer_input)\n",
    "        x = InstanceNormalization(axis=-1)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        \n",
    "        # 2nd layer\n",
    "        x = Conv2D(filters=k, kernel_size=3, strides=1, padding='same')(x)\n",
    "        x = InstanceNormalization(axis=-1)(x)\n",
    "        \n",
    "        # concatenate merge channel-wise with input layer\n",
    "        x = Concatenate()([x, layer_input])\n",
    "        return x\n",
    "    \n",
    "    # c7s1-k denote a 7×7 Convolution-InstanceNormReLU layer with k ﬁlters and stride 1\n",
    "    def c7Ak(self, layer_input, k):\n",
    "        x = Conv2D(filters=k, kernel_size=7, strides=1, padding='same')(layer_input)\n",
    "        x = InstanceNormalization(axis=-1)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "    \n",
    "    # dk denotes a 3 × 3 Convolution-InstanceNorm-ReLU layer with k ﬁlters and stride 2\n",
    "    def dk(self, layer_input, k):\n",
    "        x = Conv2D(filters=k, kernel_size=3, strides=2, padding='same')(layer_input)\n",
    "        x = InstanceNormalization(axis=-1)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "    \n",
    "    # uk denotes a 3 × 3 fractional-strided-ConvolutionInstanceNorm-ReLU layer with k ﬁlters and stride 1/2\n",
    "    def uk(self, layer_input, k):\n",
    "        # this matches fractinoally stided with stride 1/2\n",
    "        x = Conv2DTranspose(filters=k, kernel_size=3, strides=2, padding='same')(layer_input)\n",
    "        x = InstanceNormalization(axis=-1)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "     \n",
    "#===============================================================================\n",
    "# Models\n",
    "\n",
    "    # define the 70x70 patchgan discriminator model\n",
    "    def discriminator(self, name = None):\n",
    "        # Specify input\n",
    "        input_image = Input(shape=self.image_shape)\n",
    "\n",
    "        # Layer 1 (#Instance normalization is not used for this layer)\n",
    "        x = self.ck(input_image, 64, False, 2)\n",
    "        # Layer 2\n",
    "        x = self.ck(x, 128, True, 2)\n",
    "        # Layer 3\n",
    "        x = self.ck(x, 256, True, 2)\n",
    "        # Layer 4\n",
    "        x = self.ck(x, 512, True, 1)\n",
    "        \n",
    "        # Output Layer\n",
    "        if self.use_patchgan:\n",
    "            x = Conv2D(filters=1, kernel_size=4, strides=1, padding='same')(x)\n",
    "        else:\n",
    "            x = Flatten()(x)\n",
    "            x = Dense(1)(x)\n",
    "            \n",
    "        model = Model(inputs = input_image, outputs = x, name = name)\n",
    "        return model\n",
    "    \n",
    "    # 6-resnet block version\n",
    "    def generator(self, name = None):\n",
    "        # Specify input\n",
    "        input_image = Input(shape=self.image_shape)\n",
    "        \n",
    "        # Layer 1 \n",
    "        x = self.c7Ak(input_image, 64)\n",
    "        # Layer 2\n",
    "        x = self.dk(x, 128)\n",
    "        # Layer 3\n",
    "        x = self.dk(x, 256)\n",
    "        # Layer 4-9\n",
    "        for _ in range(self.n_resnet):\n",
    "            x = self.Rk(x, 256)\n",
    "        \n",
    "        # Layer 10\n",
    "        x = self.uk(x, 128)\n",
    "        # Layer 11\n",
    "        x = self.uk(x, 64)\n",
    "            \n",
    "        # Layer 12, c7s1-1\n",
    "        x = Conv2D(self.channels, kernel_size=7, strides=1, padding='same')(x)\n",
    "        x = InstanceNormalization(axis=-1)(x)\n",
    "        # pixel values are in the range [-1, 1]\n",
    "        output_image = Activation('tanh')(x) \n",
    "        \n",
    "        model = Model(inputs = input_image, outputs = output_image, name = name)\n",
    "        return model\n",
    "    \n",
    "    # For the composite model we will only train the generators\n",
    "    def composite_model(self, name = None):\n",
    "       \n",
    "        # ensure generators we're updating is trainable\n",
    "        self.G.trainable = True\n",
    "        self.F.trainable = True\n",
    "        # mark discriminator as not trainable\n",
    "        self.D_y.trainable = False\n",
    "        self.D_x.trainable = False\n",
    "        \n",
    "        # Input images from both domains\n",
    "        img_X = Input(shape=self.image_shape)\n",
    "        img_Y = Input(shape=self.image_shape)\n",
    "        \n",
    "        # Translate images to the other domain\n",
    "        fake_Y = self.G(img_X)\n",
    "        fake_X = self.F(img_Y)\n",
    "        \n",
    "        # Translate images back to original domain\n",
    "        reconstr_X = self.F(fake_Y)\n",
    "        reconstr_Y = self.G(fake_X)\n",
    "        \n",
    "        # Discriminators determines validity of translated images to compute Adversarial Loss\n",
    "        valid_Y = self.D_y(fake_Y)\n",
    "        valid_X = self.D_x(fake_X)\n",
    "        \n",
    "        model = Model(inputs=[img_X, img_Y], outputs=[valid_Y, valid_X, reconstr_X, reconstr_Y])\n",
    "        \n",
    "        return model\n",
    "        \n",
    "#===============================================================================\n",
    "# Training\n",
    "\n",
    "    def train(self):\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((self.batch_size,) + self.patch_shape)\n",
    "        fake = np.zeros((self.batch_size,) + self.patch_shape)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_i, (imgs_X, imgs_Y) in enumerate(self.load_data()):\n",
    "                \n",
    "                # ----------------------\n",
    "                #  Train Discriminators\n",
    "                # ----------------------\n",
    "                \n",
    "                fake_Y = self.G.predict(imgs_X)\n",
    "                fake_X = self.F.predict(imgs_Y)\n",
    "                \n",
    "                dY_loss_real = self.D_y.train_on_batch(imgs_Y, valid)\n",
    "                dY_loss_fake = self.D_y.train_on_batch(fake_Y, fake)\n",
    "                dY_loss = np.add(dY_loss_real, dY_loss_fake) \n",
    "                \n",
    "                dX_loss_real = self.D_x.train_on_batch(imgs_X, valid)\n",
    "                dX_loss_fake = self.D_x.train_on_batch(fake_X, fake)\n",
    "                dX_loss = np.add(dX_loss_real, dX_loss_fake) \n",
    "                \n",
    "                # Total disciminator loss\n",
    "                d_loss = 0.5 * np.add(dY_loss, dX_loss)\n",
    "                \n",
    "            \n",
    "                # ------------------\n",
    "                #  Train Generators\n",
    "                # ------------------\n",
    "                \n",
    "                g_loss = self.Composite.train_on_batch([imgs_X, imgs_Y],\n",
    "                                                             [valid, valid, imgs_X, imgs_Y])\n",
    "                \n",
    "                \n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "                \n",
    "                # Plot the progress\n",
    "                print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %05f, adversarial: %05f, reconstr: %05f] time: %s \" \\\n",
    "                                                                        % ( epoch, self.epochs,\n",
    "                                                                            batch_i, self.n_batches,\n",
    "                                                                            d_loss,\n",
    "                                                                            g_loss[0],\n",
    "                                                                            np.mean(g_loss[1:3]),\n",
    "                                                                            np.mean(g_loss[3:5]),\n",
    "                                                                            elapsed_time))\n",
    "                # If at save interval => save generated image samples\n",
    "                if batch_i % self.save_interval == 0:\n",
    "                    self.sample_images(epoch, batch_i)\n",
    "    \n",
    "#===============================================================================\n",
    "# Data Loader\n",
    "    \n",
    "    def read_image(self, img_path):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = img.resize((self.loadsize, self.loadsize), Image.BICUBIC)\n",
    "        img = np.array(img)\n",
    "        assert img.shape == (self.loadsize, self.loadsize, 3)\n",
    "        img = img.astype(np.float32)\n",
    "        img = (img - 127.5) / 127.5\n",
    "        # random jitter\n",
    "        w_offset = h_offset = randint(0, max(0, self.loadsize - self.imagesize - 1))\n",
    "        img = img[h_offset:h_offset + self.imagesize, w_offset:w_offset + self.imagesize, :]\n",
    "        # horizontal flip\n",
    "        if randint(0, 1):\n",
    "            img = img[:, ::-1]\n",
    "        return img\n",
    "        \n",
    "    def load_data(self):\n",
    "        # configure traning dataset path\n",
    "        train_A = glob.glob(self.dpath+'trainA/*')\n",
    "        train_B = glob.glob(self.dpath+'trainB/*')\n",
    "      \n",
    "        self.n_batches = int(min(len(train_A), len(train_B)) / self.batch_size)\n",
    "        total_samples = self.n_batches * self.batch_size\n",
    "        \n",
    "        # Sample n_batches * batch_size from each path list so that model sees all\n",
    "        # samples from both domains\n",
    "        train_A = np.random.choice(train_A, total_samples, replace=False)\n",
    "        train_B = np.random.choice(train_B, total_samples, replace=False)\n",
    "        \n",
    "        for i in range(self.n_batches-1):\n",
    "            batch_A = train_A[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            batch_B = train_B[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            imgs_A, imgs_B = [], []\n",
    "            for img_A, img_B in zip(batch_A, batch_B):\n",
    "                img_A = self.read_image(img_A)\n",
    "                img_B = self.read_image(img_B)\n",
    "\n",
    "                imgs_A.append(img_A)\n",
    "                imgs_B.append(img_B)\n",
    "            \n",
    "            yield np.array(imgs_A), np.array(imgs_B)\n",
    "\n",
    "#===============================================================================\n",
    "# Save samples \n",
    "    \n",
    "    def sample_images(self, epoch, batch_i):\n",
    "        os.makedirs('images/%s' % self.dpath, exist_ok = True)\n",
    "        \n",
    "        # configure testing dataset path\n",
    "        val_A = glob.glob(self.dpath+'testA/*')\n",
    "        val_B = glob.glob(self.dpath+'testB/*')\n",
    "        \n",
    "        val_A = np.random.choice(val_A, size=self.batch_size)\n",
    "        val_B = np.random.choice(val_B, size=self.batch_size)\n",
    "        \n",
    "        imgs_A, imgs_B = [], []\n",
    "        for i in range(self.batch_size):\n",
    "            path_A = val_A[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            path_B = val_B[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            for img_A, img_B in zip(path_A, path_B):\n",
    "                img_A = self.read_image(img_A)\n",
    "                img_B = self.read_image(img_B)\n",
    "\n",
    "                imgs_A.append(img_A)\n",
    "                imgs_B.append(img_B)\n",
    "        \n",
    "        imgs_A = np.array(imgs_A)\n",
    "        imgs_B = np.array(imgs_B)\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.G.predict(imgs_A)\n",
    "        fake_A = self.F.predict(imgs_B)\n",
    "        # Translate back to original domain\n",
    "        reconstr_A = self.F.predict(fake_B)\n",
    "        reconstr_B = self.G.predict(fake_A)\n",
    "        \n",
    "        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
    "        \n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        \n",
    "        titles = ['Original', 'Translated', 'Reconstructed']\n",
    "        r, c = 2, 3\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[j])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%s/%d_%d.png\" % (self.dpath, epoch, batch_i))\n",
    "        plt.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/10] [Batch 0/1800] [D loss: 1.376199] [G loss: 28.206743, adversarial: 4.365418, reconstr: 0.973795] time: 0:00:50.633677 \n",
      "[Epoch 0/10] [Batch 1/1800] [D loss: 3.479449] [G loss: 32.228142, adversarial: 8.003642, reconstr: 0.811043] time: 0:01:06.198725 \n",
      "[Epoch 0/10] [Batch 2/1800] [D loss: 6.906204] [G loss: 39.028828, adversarial: 10.182950, reconstr: 0.933146] time: 0:01:18.074673 \n",
      "[Epoch 0/10] [Batch 3/1800] [D loss: 4.470981] [G loss: 25.286594, adversarial: 4.298309, reconstr: 0.834499] time: 0:01:29.976037 \n",
      "[Epoch 0/10] [Batch 4/1800] [D loss: 2.364760] [G loss: 29.528622, adversarial: 6.264284, reconstr: 0.850003] time: 0:01:41.817020 \n",
      "[Epoch 0/10] [Batch 5/1800] [D loss: 2.090382] [G loss: 31.778677, adversarial: 6.013206, reconstr: 0.987613] time: 0:01:54.886008 \n",
      "[Epoch 0/10] [Batch 6/1800] [D loss: 1.586824] [G loss: 21.952044, adversarial: 2.173435, reconstr: 0.880259] time: 0:02:08.972531 \n",
      "[Epoch 0/10] [Batch 7/1800] [D loss: 1.330387] [G loss: 24.755527, adversarial: 3.617065, reconstr: 0.876070] time: 0:02:21.220668 \n",
      "[Epoch 0/10] [Batch 8/1800] [D loss: 3.028969] [G loss: 27.351418, adversarial: 5.286164, reconstr: 0.838954] time: 0:02:34.401760 \n",
      "[Epoch 0/10] [Batch 9/1800] [D loss: 1.526975] [G loss: 24.076212, adversarial: 3.343356, reconstr: 0.869475] time: 0:02:47.617904 \n",
      "[Epoch 0/10] [Batch 10/1800] [D loss: 0.939636] [G loss: 20.166805, adversarial: 2.055226, reconstr: 0.802818] time: 0:03:01.517149 \n",
      "[Epoch 0/10] [Batch 11/1800] [D loss: 1.403831] [G loss: 25.766766, adversarial: 4.885639, reconstr: 0.799774] time: 0:03:18.696676 \n",
      "[Epoch 0/10] [Batch 12/1800] [D loss: 1.333353] [G loss: 21.296417, adversarial: 2.663811, reconstr: 0.798440] time: 0:03:31.273482 \n",
      "[Epoch 0/10] [Batch 13/1800] [D loss: 1.621258] [G loss: 19.227678, adversarial: 1.302508, reconstr: 0.831133] time: 0:03:42.958711 \n",
      "[Epoch 0/10] [Batch 14/1800] [D loss: 0.811825] [G loss: 19.632738, adversarial: 1.666346, reconstr: 0.815002] time: 0:03:54.597726 \n",
      "[Epoch 0/10] [Batch 15/1800] [D loss: 1.068034] [G loss: 20.112556, adversarial: 2.198520, reconstr: 0.785776] time: 0:04:06.314809 \n",
      "[Epoch 0/10] [Batch 16/1800] [D loss: 1.054016] [G loss: 18.950390, adversarial: 1.161276, reconstr: 0.831392] time: 0:04:17.973921 \n",
      "[Epoch 0/10] [Batch 17/1800] [D loss: 0.834386] [G loss: 19.852812, adversarial: 1.126013, reconstr: 0.880039] time: 0:04:29.609154 \n",
      "[Epoch 0/10] [Batch 18/1800] [D loss: 0.497740] [G loss: 18.935795, adversarial: 1.066991, reconstr: 0.840091] time: 0:04:41.286579 \n",
      "[Epoch 0/10] [Batch 19/1800] [D loss: 1.042824] [G loss: 19.304403, adversarial: 1.315311, reconstr: 0.833689] time: 0:04:53.051522 \n",
      "[Epoch 0/10] [Batch 20/1800] [D loss: 0.583575] [G loss: 20.291386, adversarial: 1.239971, reconstr: 0.890572] time: 0:05:04.729653 \n",
      "[Epoch 0/10] [Batch 21/1800] [D loss: 0.557606] [G loss: 15.355431, adversarial: 0.946849, reconstr: 0.673087] time: 0:05:19.358245 \n",
      "[Epoch 0/10] [Batch 22/1800] [D loss: 0.495543] [G loss: 15.997381, adversarial: 1.208351, reconstr: 0.679034] time: 0:05:31.015225 \n",
      "[Epoch 0/10] [Batch 23/1800] [D loss: 0.591971] [G loss: 17.506496, adversarial: 1.124600, reconstr: 0.762865] time: 0:05:42.752366 \n",
      "[Epoch 0/10] [Batch 24/1800] [D loss: 0.480823] [G loss: 14.935620, adversarial: 0.883153, reconstr: 0.658466] time: 0:05:54.396128 \n",
      "[Epoch 0/10] [Batch 25/1800] [D loss: 0.485590] [G loss: 16.908230, adversarial: 1.044541, reconstr: 0.740957] time: 0:06:06.023229 \n",
      "[Epoch 0/10] [Batch 26/1800] [D loss: 0.449444] [G loss: 16.252693, adversarial: 0.957453, reconstr: 0.716889] time: 0:06:17.633005 \n",
      "[Epoch 0/10] [Batch 27/1800] [D loss: 0.333857] [G loss: 15.439891, adversarial: 1.015874, reconstr: 0.670407] time: 0:06:29.249014 \n",
      "[Epoch 0/10] [Batch 28/1800] [D loss: 0.362729] [G loss: 14.753519, adversarial: 0.876808, reconstr: 0.649995] time: 0:06:40.894737 \n",
      "[Epoch 0/10] [Batch 29/1800] [D loss: 0.354200] [G loss: 13.557075, adversarial: 0.878339, reconstr: 0.590020] time: 0:06:52.716670 \n",
      "[Epoch 0/10] [Batch 30/1800] [D loss: 0.413881] [G loss: 14.983648, adversarial: 0.959449, reconstr: 0.653238] time: 0:07:04.513522 \n",
      "[Epoch 0/10] [Batch 31/1800] [D loss: 0.410825] [G loss: 15.301134, adversarial: 1.003338, reconstr: 0.664723] time: 0:07:19.047286 \n",
      "[Epoch 0/10] [Batch 32/1800] [D loss: 0.454607] [G loss: 16.562019, adversarial: 1.606162, reconstr: 0.667485] time: 0:07:30.657093 \n",
      "[Epoch 0/10] [Batch 33/1800] [D loss: 0.614089] [G loss: 20.735432, adversarial: 1.284210, reconstr: 0.908351] time: 0:07:42.416461 \n",
      "[Epoch 0/10] [Batch 34/1800] [D loss: 0.616347] [G loss: 15.344415, adversarial: 1.116074, reconstr: 0.655613] time: 0:07:54.111277 \n",
      "[Epoch 0/10] [Batch 35/1800] [D loss: 0.675365] [G loss: 13.313467, adversarial: 1.019900, reconstr: 0.563683] time: 0:08:05.864477 \n",
      "[Epoch 0/10] [Batch 36/1800] [D loss: 0.551511] [G loss: 15.102473, adversarial: 0.830039, reconstr: 0.672120] time: 0:08:17.643608 \n",
      "[Epoch 0/10] [Batch 37/1800] [D loss: 0.505793] [G loss: 19.450680, adversarial: 1.074672, reconstr: 0.865067] time: 0:08:29.258672 \n",
      "[Epoch 0/10] [Batch 38/1800] [D loss: 0.513679] [G loss: 14.936192, adversarial: 1.025901, reconstr: 0.644219] time: 0:08:40.990182 \n",
      "[Epoch 0/10] [Batch 39/1800] [D loss: 0.345177] [G loss: 17.469421, adversarial: 0.916094, reconstr: 0.781862] time: 0:08:52.745906 \n",
      "[Epoch 0/10] [Batch 40/1800] [D loss: 0.438717] [G loss: 15.506754, adversarial: 0.997086, reconstr: 0.675629] time: 0:09:04.503352 \n",
      "[Epoch 0/10] [Batch 41/1800] [D loss: 0.444367] [G loss: 15.372437, adversarial: 0.835295, reconstr: 0.685092] time: 0:09:19.080292 \n",
      "[Epoch 0/10] [Batch 42/1800] [D loss: 0.301761] [G loss: 13.576229, adversarial: 0.737376, reconstr: 0.605074] time: 0:09:30.772932 \n",
      "[Epoch 0/10] [Batch 43/1800] [D loss: 0.400999] [G loss: 17.921024, adversarial: 0.814854, reconstr: 0.814566] time: 0:09:42.384929 \n",
      "[Epoch 0/10] [Batch 44/1800] [D loss: 0.401598] [G loss: 15.291851, adversarial: 0.940320, reconstr: 0.670560] time: 0:09:54.129702 \n",
      "[Epoch 0/10] [Batch 45/1800] [D loss: 0.416161] [G loss: 13.035979, adversarial: 0.797505, reconstr: 0.572048] time: 0:10:05.965146 \n",
      "[Epoch 0/10] [Batch 46/1800] [D loss: 0.318698] [G loss: 15.149431, adversarial: 0.780417, reconstr: 0.679430] time: 0:10:17.573942 \n",
      "[Epoch 0/10] [Batch 47/1800] [D loss: 0.336940] [G loss: 12.772462, adversarial: 0.821620, reconstr: 0.556461] time: 0:10:29.343039 \n",
      "[Epoch 0/10] [Batch 48/1800] [D loss: 0.366067] [G loss: 12.367550, adversarial: 0.780970, reconstr: 0.540281] time: 0:10:40.891242 \n",
      "[Epoch 0/10] [Batch 49/1800] [D loss: 0.305660] [G loss: 15.376118, adversarial: 0.897675, reconstr: 0.679038] time: 0:10:52.617250 \n",
      "[Epoch 0/10] [Batch 50/1800] [D loss: 0.254773] [G loss: 18.733322, adversarial: 0.894937, reconstr: 0.847172] time: 0:11:04.263489 \n",
      "[Epoch 0/10] [Batch 51/1800] [D loss: 0.313677] [G loss: 18.386711, adversarial: 0.925808, reconstr: 0.826755] time: 0:11:18.773798 \n",
      "[Epoch 0/10] [Batch 52/1800] [D loss: 0.295512] [G loss: 13.390116, adversarial: 0.997644, reconstr: 0.569741] time: 0:11:30.397188 \n",
      "[Epoch 0/10] [Batch 53/1800] [D loss: 0.304207] [G loss: 17.232868, adversarial: 0.901388, reconstr: 0.771505] time: 0:11:42.120230 \n",
      "[Epoch 0/10] [Batch 54/1800] [D loss: 0.382844] [G loss: 15.310862, adversarial: 1.074722, reconstr: 0.658071] time: 0:11:53.771927 \n",
      "[Epoch 0/10] [Batch 55/1800] [D loss: 0.418024] [G loss: 13.337439, adversarial: 0.839110, reconstr: 0.582961] time: 0:12:05.447319 \n",
      "[Epoch 0/10] [Batch 56/1800] [D loss: 0.268384] [G loss: 11.772977, adversarial: 0.784378, reconstr: 0.510211] time: 0:12:17.295877 \n",
      "[Epoch 0/10] [Batch 57/1800] [D loss: 0.315354] [G loss: 15.820703, adversarial: 0.866910, reconstr: 0.704344] time: 0:12:29.042548 \n",
      "[Epoch 0/10] [Batch 58/1800] [D loss: 0.453351] [G loss: 16.608334, adversarial: 0.698738, reconstr: 0.760543] time: 0:12:40.633547 \n",
      "[Epoch 0/10] [Batch 59/1800] [D loss: 0.299462] [G loss: 13.404648, adversarial: 0.860029, reconstr: 0.584229] time: 0:12:52.402848 \n",
      "[Epoch 0/10] [Batch 60/1800] [D loss: 0.344185] [G loss: 11.362680, adversarial: 0.705988, reconstr: 0.497535] time: 0:13:04.157949 \n",
      "[Epoch 0/10] [Batch 61/1800] [D loss: 0.200919] [G loss: 15.535576, adversarial: 0.915628, reconstr: 0.685216] time: 0:13:18.855683 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/10] [Batch 62/1800] [D loss: 0.288570] [G loss: 13.021157, adversarial: 0.871941, reconstr: 0.563864] time: 0:13:30.958693 \n",
      "[Epoch 0/10] [Batch 63/1800] [D loss: 0.274733] [G loss: 16.413179, adversarial: 0.922105, reconstr: 0.728449] time: 0:13:42.778239 \n",
      "[Epoch 0/10] [Batch 64/1800] [D loss: 0.241543] [G loss: 12.707635, adversarial: 0.791461, reconstr: 0.556236] time: 0:13:54.192066 \n",
      "[Epoch 0/10] [Batch 65/1800] [D loss: 0.243782] [G loss: 13.345842, adversarial: 0.784290, reconstr: 0.588863] time: 0:15:53.944917 \n",
      "[Epoch 0/10] [Batch 66/1800] [D loss: 0.217156] [G loss: 11.424251, adversarial: 0.901076, reconstr: 0.481105] time: 0:16:05.440400 \n",
      "[Epoch 0/10] [Batch 67/1800] [D loss: 0.240669] [G loss: 13.034932, adversarial: 1.138720, reconstr: 0.537875] time: 0:16:16.893259 \n",
      "[Epoch 0/10] [Batch 68/1800] [D loss: 0.274508] [G loss: 13.636000, adversarial: 0.829706, reconstr: 0.598829] time: 0:29:10.263790 \n",
      "[Epoch 0/10] [Batch 69/1800] [D loss: 0.287220] [G loss: 14.618345, adversarial: 0.832221, reconstr: 0.647695] time: 0:29:26.416824 \n",
      "[Epoch 0/10] [Batch 70/1800] [D loss: 0.202073] [G loss: 12.107897, adversarial: 0.877942, reconstr: 0.517601] time: 0:29:39.645509 \n",
      "[Epoch 0/10] [Batch 71/1800] [D loss: 0.216793] [G loss: 13.057693, adversarial: 0.887963, reconstr: 0.564088] time: 0:29:55.354973 \n",
      "[Epoch 0/10] [Batch 72/1800] [D loss: 0.230834] [G loss: 13.020144, adversarial: 0.831325, reconstr: 0.567875] time: 0:30:08.741394 \n",
      "[Epoch 0/10] [Batch 73/1800] [D loss: 0.195264] [G loss: 12.610085, adversarial: 0.826506, reconstr: 0.547854] time: 0:30:21.159379 \n",
      "[Epoch 0/10] [Batch 74/1800] [D loss: 0.232396] [G loss: 15.499371, adversarial: 0.887868, reconstr: 0.686182] time: 0:30:34.254458 \n",
      "[Epoch 0/10] [Batch 75/1800] [D loss: 0.260151] [G loss: 13.065796, adversarial: 0.883434, reconstr: 0.564946] time: 0:30:46.032135 \n",
      "[Epoch 0/10] [Batch 76/1800] [D loss: 0.286765] [G loss: 12.724246, adversarial: 1.101125, reconstr: 0.526100] time: 0:30:57.661759 \n",
      "[Epoch 0/10] [Batch 77/1800] [D loss: 0.323085] [G loss: 14.327321, adversarial: 1.105557, reconstr: 0.605810] time: 0:31:09.489808 \n",
      "[Epoch 0/10] [Batch 78/1800] [D loss: 0.275564] [G loss: 15.571651, adversarial: 0.881939, reconstr: 0.690389] time: 0:31:21.171177 \n",
      "[Epoch 0/10] [Batch 79/1800] [D loss: 0.223429] [G loss: 12.092197, adversarial: 0.805910, reconstr: 0.524019] time: 0:31:32.908743 \n",
      "[Epoch 0/10] [Batch 80/1800] [D loss: 0.226871] [G loss: 15.715219, adversarial: 0.836007, reconstr: 0.702160] time: 0:31:44.549122 \n",
      "[Epoch 0/10] [Batch 81/1800] [D loss: 0.220897] [G loss: 13.114662, adversarial: 0.765523, reconstr: 0.579181] time: 0:31:59.172654 \n",
      "[Epoch 0/10] [Batch 82/1800] [D loss: 0.296802] [G loss: 12.237042, adversarial: 0.785497, reconstr: 0.533302] time: 0:32:10.967943 \n",
      "[Epoch 0/10] [Batch 83/1800] [D loss: 0.222599] [G loss: 12.629868, adversarial: 0.861488, reconstr: 0.545345] time: 0:32:22.688110 \n",
      "[Epoch 0/10] [Batch 84/1800] [D loss: 0.254875] [G loss: 17.575008, adversarial: 1.258618, reconstr: 0.752889] time: 0:32:34.311178 \n",
      "[Epoch 0/10] [Batch 85/1800] [D loss: 0.359614] [G loss: 16.949341, adversarial: 0.982514, reconstr: 0.749216] time: 0:32:45.909177 \n",
      "[Epoch 0/10] [Batch 86/1800] [D loss: 0.382503] [G loss: 14.763359, adversarial: 1.037134, reconstr: 0.634455] time: 0:32:57.520213 \n",
      "[Epoch 0/10] [Batch 87/1800] [D loss: 0.190982] [G loss: 14.490353, adversarial: 0.818335, reconstr: 0.642684] time: 0:33:09.365179 \n",
      "[Epoch 0/10] [Batch 88/1800] [D loss: 0.389491] [G loss: 14.019365, adversarial: 0.874188, reconstr: 0.613549] time: 0:33:20.943308 \n",
      "[Epoch 0/10] [Batch 89/1800] [D loss: 0.286072] [G loss: 12.569515, adversarial: 1.136994, reconstr: 0.514776] time: 0:33:32.607957 \n",
      "[Epoch 0/10] [Batch 90/1800] [D loss: 0.501899] [G loss: 14.250146, adversarial: 0.923488, reconstr: 0.620158] time: 0:33:44.242858 \n",
      "[Epoch 0/10] [Batch 91/1800] [D loss: 0.419117] [G loss: 14.941788, adversarial: 0.831786, reconstr: 0.663911] time: 0:33:58.800351 \n",
      "[Epoch 0/10] [Batch 92/1800] [D loss: 0.342524] [G loss: 13.167030, adversarial: 1.192629, reconstr: 0.539089] time: 0:34:10.475263 \n",
      "[Epoch 0/10] [Batch 93/1800] [D loss: 0.251386] [G loss: 19.480844, adversarial: 1.102487, reconstr: 0.863793] time: 0:34:22.094162 \n",
      "[Epoch 0/10] [Batch 94/1800] [D loss: 0.420857] [G loss: 13.941440, adversarial: 0.843582, reconstr: 0.612714] time: 0:34:33.689560 \n",
      "[Epoch 0/10] [Batch 95/1800] [D loss: 0.240228] [G loss: 16.372715, adversarial: 0.910881, reconstr: 0.727548] time: 0:34:45.311622 \n",
      "[Epoch 0/10] [Batch 96/1800] [D loss: 0.256097] [G loss: 14.241526, adversarial: 0.784145, reconstr: 0.633662] time: 0:34:56.952542 \n",
      "[Epoch 0/10] [Batch 97/1800] [D loss: 0.210274] [G loss: 15.152918, adversarial: 0.833332, reconstr: 0.674313] time: 0:35:08.661162 \n",
      "[Epoch 0/10] [Batch 98/1800] [D loss: 0.147998] [G loss: 13.499144, adversarial: 0.921202, reconstr: 0.582837] time: 0:35:20.277084 \n",
      "[Epoch 0/10] [Batch 99/1800] [D loss: 0.186990] [G loss: 13.036194, adversarial: 0.841920, reconstr: 0.567618] time: 0:35:32.047073 \n",
      "[Epoch 0/10] [Batch 100/1800] [D loss: 0.241017] [G loss: 12.988198, adversarial: 0.814409, reconstr: 0.567969] time: 0:35:43.684232 \n",
      "[Epoch 0/10] [Batch 101/1800] [D loss: 0.197069] [G loss: 17.993263, adversarial: 0.887349, reconstr: 0.810928] time: 0:35:58.268785 \n",
      "[Epoch 0/10] [Batch 102/1800] [D loss: 0.205995] [G loss: 13.243103, adversarial: 0.874585, reconstr: 0.574697] time: 0:36:09.944388 \n",
      "[Epoch 0/10] [Batch 103/1800] [D loss: 0.125247] [G loss: 17.903534, adversarial: 0.962867, reconstr: 0.798890] time: 0:36:21.600996 \n",
      "[Epoch 0/10] [Batch 104/1800] [D loss: 0.295737] [G loss: 14.871110, adversarial: 0.802486, reconstr: 0.663307] time: 0:36:33.184367 \n",
      "[Epoch 0/10] [Batch 105/1800] [D loss: 0.257830] [G loss: 12.282274, adversarial: 0.841062, reconstr: 0.530007] time: 0:36:44.785127 \n",
      "[Epoch 0/10] [Batch 106/1800] [D loss: 0.242717] [G loss: 12.702261, adversarial: 0.954897, reconstr: 0.539623] time: 0:36:56.526396 \n",
      "[Epoch 0/10] [Batch 107/1800] [D loss: 0.324716] [G loss: 13.041594, adversarial: 0.899875, reconstr: 0.562092] time: 0:37:08.440013 \n",
      "[Epoch 0/10] [Batch 108/1800] [D loss: 0.218973] [G loss: 11.453396, adversarial: 0.896705, reconstr: 0.482999] time: 0:37:20.011739 \n",
      "[Epoch 0/10] [Batch 109/1800] [D loss: 0.230514] [G loss: 18.194679, adversarial: 1.280108, reconstr: 0.781723] time: 0:37:31.670270 \n",
      "[Epoch 0/10] [Batch 110/1800] [D loss: 0.491851] [G loss: 16.377903, adversarial: 0.754067, reconstr: 0.743488] time: 0:37:43.190981 \n",
      "[Epoch 0/10] [Batch 111/1800] [D loss: 0.269333] [G loss: 15.419674, adversarial: 0.900888, reconstr: 0.680895] time: 0:37:57.708424 \n",
      "[Epoch 0/10] [Batch 112/1800] [D loss: 0.222812] [G loss: 13.169280, adversarial: 0.815938, reconstr: 0.576870] time: 0:38:09.380155 \n",
      "[Epoch 0/10] [Batch 113/1800] [D loss: 0.210263] [G loss: 14.077409, adversarial: 0.881115, reconstr: 0.615759] time: 0:38:21.032358 \n",
      "[Epoch 0/10] [Batch 114/1800] [D loss: 0.283436] [G loss: 14.810894, adversarial: 0.821252, reconstr: 0.658419] time: 0:38:32.720452 \n",
      "[Epoch 0/10] [Batch 115/1800] [D loss: 0.200603] [G loss: 15.881035, adversarial: 0.821278, reconstr: 0.711924] time: 0:38:44.480157 \n",
      "[Epoch 0/10] [Batch 116/1800] [D loss: 0.180620] [G loss: 15.789798, adversarial: 0.802790, reconstr: 0.709211] time: 0:38:56.204834 \n",
      "[Epoch 0/10] [Batch 117/1800] [D loss: 0.132952] [G loss: 16.545429, adversarial: 0.876987, reconstr: 0.739573] time: 0:39:08.003238 \n",
      "[Epoch 0/10] [Batch 118/1800] [D loss: 0.225427] [G loss: 13.065224, adversarial: 0.871459, reconstr: 0.566115] time: 0:39:19.676108 \n",
      "[Epoch 0/10] [Batch 119/1800] [D loss: 0.166990] [G loss: 12.234431, adversarial: 0.892183, reconstr: 0.522503] time: 0:39:31.290247 \n",
      "[Epoch 0/10] [Batch 120/1800] [D loss: 0.197204] [G loss: 19.635086, adversarial: 1.349414, reconstr: 0.846813] time: 0:39:42.989726 \n",
      "[Epoch 0/10] [Batch 121/1800] [D loss: 0.347999] [G loss: 16.215054, adversarial: 0.865703, reconstr: 0.724182] time: 0:39:57.518601 \n",
      "[Epoch 0/10] [Batch 122/1800] [D loss: 0.288453] [G loss: 13.520802, adversarial: 0.812692, reconstr: 0.594771] time: 0:40:09.320077 \n",
      "[Epoch 0/10] [Batch 123/1800] [D loss: 0.236493] [G loss: 11.821857, adversarial: 0.817971, reconstr: 0.509296] time: 0:40:20.959170 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/10] [Batch 124/1800] [D loss: 0.238854] [G loss: 12.333911, adversarial: 0.802552, reconstr: 0.536440] time: 0:40:32.714788 \n",
      "[Epoch 0/10] [Batch 125/1800] [D loss: 0.183476] [G loss: 13.541941, adversarial: 0.785573, reconstr: 0.598540] time: 0:40:44.733222 \n",
      "[Epoch 0/10] [Batch 126/1800] [D loss: 0.219236] [G loss: 12.405373, adversarial: 0.862732, reconstr: 0.533995] time: 0:40:56.509529 \n",
      "[Epoch 0/10] [Batch 127/1800] [D loss: 0.172206] [G loss: 11.896006, adversarial: 0.836801, reconstr: 0.511120] time: 0:41:08.076407 \n",
      "[Epoch 0/10] [Batch 128/1800] [D loss: 0.295502] [G loss: 12.849864, adversarial: 0.803123, reconstr: 0.562181] time: 0:42:22.460940 \n",
      "[Epoch 0/10] [Batch 129/1800] [D loss: 0.175286] [G loss: 13.776447, adversarial: 0.927267, reconstr: 0.596096] time: 0:42:33.839549 \n",
      "[Epoch 0/10] [Batch 130/1800] [D loss: 0.192873] [G loss: 11.111664, adversarial: 0.822597, reconstr: 0.473324] time: 0:42:47.685347 \n",
      "[Epoch 0/10] [Batch 131/1800] [D loss: 0.125216] [G loss: 16.142044, adversarial: 0.935123, reconstr: 0.713590] time: 0:51:39.178422 \n",
      "[Epoch 0/10] [Batch 132/1800] [D loss: 0.239330] [G loss: 14.109503, adversarial: 0.836588, reconstr: 0.621816] time: 0:51:52.164083 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c08199ec017b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# # create the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mGAN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCycleGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# model = GAN.Composite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# # summarize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-8cf61c2dd34e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 g_loss = self.Composite.train_on_batch([imgs_X, imgs_Y],\n\u001b[0;32m--> 245\u001b[0;31m                                                              [valid, valid, imgs_X, imgs_Y])\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # create the model\n",
    "GAN = CycleGAN()\n",
    "GAN.train()\n",
    "# model = GAN.Composite\n",
    "# # summarize the model\n",
    "# model.summary()\n",
    "# # plot the model\n",
    "# plot_model(model, to_file='composite_model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
